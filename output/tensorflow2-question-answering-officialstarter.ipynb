{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Tensorflow 2.0 with Bert on Natural Questions\n",
    "\n",
    "In this notebook, we'll be using the Bert baseline for Tensorflow to create predictions for the Natural Questions test set. Note that this uses a model that has already been pre-trained - we're only doing inference here. A GPU is required, and this should take between 1-2 hours to run.\n",
    "\n",
    "The original script can be found [here](https://github.com/google-research/language/blob/master/language/question_answering/bert_joint/run_nq.py).\n",
    "The supporting modules were drawn from the [official Tensorflow model repository](https://github.com/tensorflow/models/tree/master/official/nlp). The bert-joint-baseline data is described [here](https://github.com/google-research/language/tree/master/language/question_answering/bert_joint).\n",
    "\n",
    "**Note:** This baseline uses code that was migrated from TF1.x. Be aware that it contains use of tf.compat.v1, which is not permitted to be eligible for [TF2.0 prizes in this competition](https://www.kaggle.com/c/tensorflow2-question-answering/overview/prizes). It is intended to be used as a starting point, but we're excited to see how much better you can do using TF2.0!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/bertjointbaseline/bert_joint.ckpt.index\n",
      "/kaggle/input/bertjointbaseline/vocab-nq.txt\n",
      "/kaggle/input/bertjointbaseline/nq-train.tfrecords-00000-of-00001\n",
      "/kaggle/input/bertjointbaseline/bert_joint.ckpt.data-00000-of-00001\n",
      "/kaggle/input/bertjointbaseline/bert_config.json\n",
      "/kaggle/input/tensorflow2-question-answering/simplified-nq-train.jsonl\n",
      "/kaggle/input/tensorflow2-question-answering/simplified-nq-test.jsonl\n",
      "/kaggle/input/tensorflow2-question-answering/sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "import tf2_0_baseline_w_bert as tf2baseline\n",
    "import bert_modeling as modeling\n",
    "import bert_optimization as optimization\n",
    "import bert_tokenization as tokenization\n",
    "import json\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "# In this case, we've got some extra BERT model files under `/kaggle/input/bertjointbaseline`\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorflow flags are variables that can be passed around within the TF system. Every flag below has some context provided regarding what the flag is and how it's used.\n",
    "\n",
    "#### Most of these can be changed as desired, with the exception of the Special Flags at the bottom, which _must_ stay as-is to work with the Kaggle back end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()\n",
    "    keys_list = [keys for keys in flags_dict]\n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "\n",
    "del_all_flags(tf.compat.v1.app.flags.FLAGS)\n",
    "\n",
    "flags = tf.compat.v1.app.flags\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"bert_config_file\", \"/kaggle/input/bertjointbaseline/bert_config.json\",\n",
    "    \"The config json file corresponding to the pre-trained BERT model. \"\n",
    "    \"This specifies the model architecture.\")\n",
    "\n",
    "flags.DEFINE_string(\"vocab_file\", \"/kaggle/input/bertjointbaseline/vocab-nq.txt\",\n",
    "                    \"The vocabulary file that the BERT model was trained on.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_dir\", \"outdir\",\n",
    "    \"The output directory where the model checkpoints will be written.\")\n",
    "\n",
    "flags.DEFINE_string(\"train_precomputed_file\", None,\n",
    "                    \"Precomputed tf records for training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"train_num_precomputed\", None,\n",
    "                     \"Number of precomputed tf records for training.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"output_prediction_file\", \"predictions.json\",\n",
    "    \"Where to print predictions in NQ prediction format, to be passed to\"\n",
    "    \"natural_questions.nq_eval.\")\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"init_checkpoint\", \"/kaggle/input/bertjointbaseline/bert_joint.ckpt\",\n",
    "    \"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"do_lower_case\", True,\n",
    "    \"Whether to lower case the input text. Should be True for uncased \"\n",
    "    \"models and False for cased models.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_seq_length\", 384,\n",
    "    \"The maximum total input sequence length after WordPiece tokenization. \"\n",
    "    \"Sequences longer than this will be truncated, and sequences shorter \"\n",
    "    \"than this will be padded.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"doc_stride\", 128,\n",
    "    \"When splitting up a long document into chunks, how much stride to \"\n",
    "    \"take between chunks.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_query_length\", 64,\n",
    "    \"The maximum number of tokens for the question. Questions longer than \"\n",
    "    \"this will be truncated to this length.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_train\", False, \"Whether to run training.\")\n",
    "\n",
    "flags.DEFINE_bool(\"do_predict\", True, \"Whether to run eval on the dev set.\")\n",
    "\n",
    "flags.DEFINE_integer(\"train_batch_size\", 32, \"Total batch size for training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"predict_batch_size\", 8,\n",
    "                     \"Total batch size for predictions.\")\n",
    "\n",
    "flags.DEFINE_float(\"learning_rate\", 5e-5, \"The initial learning rate for Adam.\")\n",
    "\n",
    "flags.DEFINE_float(\"num_train_epochs\", 3.0,\n",
    "                   \"Total number of training epochs to perform.\")\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    \"warmup_proportion\", 0.1,\n",
    "    \"Proportion of training to perform linear learning rate warmup for. \"\n",
    "    \"E.g., 0.1 = 10% of training.\")\n",
    "\n",
    "flags.DEFINE_integer(\"save_checkpoints_steps\", 1000,\n",
    "                     \"How often to save the model checkpoint.\")\n",
    "\n",
    "flags.DEFINE_integer(\"iterations_per_loop\", 1000,\n",
    "                     \"How many steps to make in each estimator call.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"n_best_size\", 20,\n",
    "    \"The total number of n-best predictions to generate in the \"\n",
    "    \"nbest_predictions.json output file.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"verbosity\", 1, \"How verbose our error messages should be\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_answer_length\", 30,\n",
    "    \"The maximum length of an answer that can be generated. This is needed \"\n",
    "    \"because the start and end predictions are not conditioned on one another.\")\n",
    "\n",
    "flags.DEFINE_float(\n",
    "    \"include_unknowns\", -1.0,\n",
    "    \"If positive, probability of including answers of type `UNKNOWN`.\")\n",
    "\n",
    "flags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU/CPU.\")\n",
    "\n",
    "tf.compat.v1.flags.DEFINE_string(\n",
    "    \"tpu_name\", None,\n",
    "    \"The Cloud TPU to use for training. This should be either the name \"\n",
    "    \"used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 \"\n",
    "    \"url.\")\n",
    "\n",
    "tf.compat.v1.flags.DEFINE_string(\n",
    "    \"tpu_zone\", None,\n",
    "    \"[Optional] GCE zone where the Cloud TPU is located in. If not \"\n",
    "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
    "    \"metadata.\")\n",
    "\n",
    "tf.compat.v1.flags.DEFINE_string(\n",
    "    \"gcp_project\", None,\n",
    "    \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\n",
    "    \"specified, we will attempt to automatically detect the GCE project from \"\n",
    "    \"metadata.\")\n",
    "\n",
    "tf.compat.v1.flags.DEFINE_string(\"master\", None, \"[Optional] TensorFlow master URL.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"num_tpu_cores\", 8,\n",
    "    \"Only used if `use_tpu` is True. Total number of TPU cores to use.\")\n",
    "\n",
    "flags.DEFINE_bool(\n",
    "    \"verbose_logging\", False,\n",
    "    \"If true, all of the warnings related to data processing will be printed. \"\n",
    "    \"A number of warnings are expected for a normal NQ evaluation.\")\n",
    "\n",
    "flags.DEFINE_boolean(\n",
    "    \"skip_nested_contexts\", True,\n",
    "    \"Completely ignore context that are not top level nodes in the page.\")\n",
    "\n",
    "flags.DEFINE_integer(\"task_id\", 0,\n",
    "                     \"Train and dev shard to read from and write to.\")\n",
    "\n",
    "flags.DEFINE_integer(\"max_contexts\", 48,\n",
    "                     \"Maximum number of contexts to output for an example.\")\n",
    "\n",
    "flags.DEFINE_integer(\n",
    "    \"max_position\", 50,\n",
    "    \"Maximum context position for which to generate special tokens.\")\n",
    "\n",
    "\n",
    "## Special flags - do not change\n",
    "\n",
    "flags.DEFINE_string(\n",
    "    \"predict_file\", \"/kaggle/input/tensorflow2-question-answering/simplified-nq-test.jsonl\",\n",
    "    \"NQ json for predictions. E.g., dev-v1.1.jsonl.gz or test-v1.1.jsonl.gz\")\n",
    "flags.DEFINE_boolean(\"logtostderr\", True, \"Logs to stderr\")\n",
    "flags.DEFINE_boolean(\"undefok\", True, \"it's okay to be undefined\")\n",
    "flags.DEFINE_string('f', '', 'kernel')\n",
    "flags.DEFINE_string('HistoryManager.hist_file', '', 'kernel')\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, we:\n",
    "1. Set up Bert\n",
    "2. Read in the test set\n",
    "3. Run it past the pre-built Bert model to create embeddings\n",
    "4. Use those embeddings to make predictions\n",
    "5. Write those predictions to `predictions.json`\n",
    "\n",
    "Feel free to change the code below. Code for the `tf2baseline.*` functions is included in the `tf2_0_baseline_w_bert` utility script, and can be customized, whether by forking the utility script and updating it, or by creating your own non-`tf2baseline` versions in this kernel.\n",
    "\n",
    "Note: the `tf2_0_baseline_w_bert` utility script contains code for training your own embeddings. Here that code is removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLAGS.predict_file /kaggle/input/tensorflow2-question-answering/simplified-nq-test.jsonl\n",
      "***** Running predictions *****\n",
      "  Num orig examples = 346\n",
      "  Num split examples = 9409\n",
      "  Batch size = 8\n",
      "  Num split into 3 = 8\n",
      "  Num split into 19 = 9\n",
      "  Num split into 50 = 5\n",
      "  Num split into 2 = 6\n",
      "  Num split into 34 = 6\n",
      "  Num split into 54 = 1\n",
      "  Num split into 40 = 7\n",
      "  Num split into 42 = 3\n",
      "  Num split into 22 = 7\n",
      "  Num split into 11 = 12\n",
      "  Num split into 29 = 8\n",
      "  Num split into 102 = 1\n",
      "  Num split into 60 = 3\n",
      "  Num split into 10 = 12\n",
      "  Num split into 21 = 6\n",
      "  Num split into 41 = 4\n",
      "  Num split into 6 = 7\n",
      "  Num split into 35 = 8\n",
      "  Num split into 23 = 4\n",
      "  Num split into 32 = 7\n",
      "  Num split into 17 = 10\n",
      "  Num split into 85 = 1\n",
      "  Num split into 30 = 6\n",
      "  Num split into 9 = 8\n",
      "  Num split into 1 = 7\n",
      "  Num split into 57 = 3\n",
      "  Num split into 5 = 9\n",
      "  Num split into 28 = 5\n",
      "  Num split into 31 = 7\n",
      "  Num split into 18 = 6\n",
      "  Num split into 47 = 5\n",
      "  Num split into 4 = 12\n",
      "  Num split into 67 = 1\n",
      "  Num split into 45 = 4\n",
      "  Num split into 27 = 7\n",
      "  Num split into 8 = 9\n",
      "  Num split into 63 = 1\n",
      "  Num split into 43 = 5\n",
      "  Num split into 13 = 9\n",
      "  Num split into 12 = 6\n",
      "  Num split into 16 = 6\n",
      "  Num split into 24 = 2\n",
      "  Num split into 14 = 4\n",
      "  Num split into 53 = 4\n",
      "  Num split into 20 = 5\n",
      "  Num split into 15 = 6\n",
      "  Num split into 7 = 6\n",
      "  Num split into 44 = 2\n",
      "  Num split into 112 = 1\n",
      "  Num split into 37 = 5\n",
      "  Num split into 46 = 3\n",
      "  Num split into 39 = 5\n",
      "  Num split into 87 = 1\n",
      "  Num split into 48 = 1\n",
      "  Num split into 33 = 2\n",
      "  Num split into 66 = 1\n",
      "  Num split into 49 = 3\n",
      "  Num split into 62 = 2\n",
      "  Num split into 125 = 1\n",
      "  Num split into 36 = 6\n",
      "  Num split into 26 = 8\n",
      "  Num split into 76 = 2\n",
      "  Num split into 121 = 1\n",
      "  Num split into 38 = 6\n",
      "  Num split into 55 = 1\n",
      "  Num split into 25 = 7\n",
      "  Num split into 56 = 3\n",
      "  Num split into 82 = 1\n",
      "  Num split into 58 = 1\n",
      "  Num split into 98 = 1\n",
      "  Num split into 52 = 1\n",
      "  Num split into 89 = 1\n",
      "  Num split into 73 = 1\n",
      "  Num split into 187 = 1\n",
      "outdir/eval.tf_record\n",
      "Processing example: 0\n",
      "Processing example: 1000\n",
      "Processing example: 2000\n",
      "Processing example: 3000\n",
      "Processing example: 4000\n",
      "Processing example: 5000\n",
      "Processing example: 6000\n",
      "Processing example: 7000\n",
      "Processing example: 8000\n",
      "Processing example: 9000\n",
      "Going to candidates file\n",
      "setting up eval features\n",
      "compute_pred_dict\n",
      "Examples processed: 100\n",
      "Examples processed: 200\n",
      "Examples processed: 300\n",
      "writing json\n"
     ]
    }
   ],
   "source": [
    "bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\n",
    "\n",
    "tf2baseline.validate_flags_or_throw(bert_config)\n",
    "tf.io.gfile.makedirs(FLAGS.output_dir)\n",
    "\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "    vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n",
    "\n",
    "tpu_cluster_resolver = None\n",
    "if FLAGS.use_tpu and FLAGS.tpu_name:\n",
    "  tpu_cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n",
    "      FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n",
    "\n",
    "is_per_host = tf.compat.v1.estimator.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "run_config = tf.compat.v1.estimator.tpu.RunConfig(\n",
    "    cluster=tpu_cluster_resolver,\n",
    "    master=FLAGS.master,\n",
    "    model_dir=FLAGS.output_dir,\n",
    "    save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n",
    "    tpu_config=tf.compat.v1.estimator.tpu.TPUConfig(\n",
    "        iterations_per_loop=FLAGS.iterations_per_loop,\n",
    "        num_shards=FLAGS.num_tpu_cores,\n",
    "        per_host_input_for_training=is_per_host))\n",
    "\n",
    "num_train_steps = None\n",
    "num_warmup_steps = None\n",
    "\n",
    "model_fn = tf2baseline.model_fn_builder(\n",
    "    bert_config=bert_config,\n",
    "    init_checkpoint=FLAGS.init_checkpoint,\n",
    "    learning_rate=FLAGS.learning_rate,\n",
    "    num_train_steps=num_train_steps,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    use_tpu=FLAGS.use_tpu,\n",
    "    use_one_hot_embeddings=FLAGS.use_tpu)\n",
    "\n",
    "# If TPU is not available, this falls back to normal Estimator on CPU or GPU.\n",
    "estimator = tf.compat.v1.estimator.tpu.TPUEstimator(\n",
    "    use_tpu=FLAGS.use_tpu,\n",
    "    model_fn=model_fn,\n",
    "    config=run_config,\n",
    "    train_batch_size=FLAGS.train_batch_size,\n",
    "    predict_batch_size=FLAGS.predict_batch_size)\n",
    "\n",
    "if FLAGS.do_predict:\n",
    "  if not FLAGS.output_prediction_file:\n",
    "    raise ValueError(\n",
    "        \"--output_prediction_file must be defined in predict mode.\")\n",
    "    \n",
    "  eval_examples = tf2baseline.read_nq_examples(\n",
    "      input_file=FLAGS.predict_file, is_training=False)\n",
    "\n",
    "  print(\"FLAGS.predict_file\", FLAGS.predict_file)\n",
    "\n",
    "  eval_writer = tf2baseline.FeatureWriter(\n",
    "      filename=os.path.join(FLAGS.output_dir, \"eval.tf_record\"),\n",
    "      is_training=False)\n",
    "  eval_features = []\n",
    "\n",
    "  def append_feature(feature):\n",
    "    eval_features.append(feature)\n",
    "    eval_writer.process_feature(feature)\n",
    "\n",
    "  num_spans_to_ids = tf2baseline.convert_examples_to_features(\n",
    "      examples=eval_examples,\n",
    "      tokenizer=tokenizer,\n",
    "      is_training=False,\n",
    "      output_fn=append_feature)\n",
    "  eval_writer.close()\n",
    "  eval_filename = eval_writer.filename\n",
    "\n",
    "  print(\"***** Running predictions *****\")\n",
    "  print(f\"  Num orig examples = %d\" % len(eval_examples))\n",
    "  print(f\"  Num split examples = %d\" % len(eval_features))\n",
    "  print(f\"  Batch size = %d\" % FLAGS.predict_batch_size)\n",
    "  for spans, ids in num_spans_to_ids.items():\n",
    "    print(f\"  Num split into %d = %d\" % (spans, len(ids)))\n",
    "\n",
    "  predict_input_fn = tf2baseline.input_fn_builder(\n",
    "      input_file=eval_filename,\n",
    "      seq_length=FLAGS.max_seq_length,\n",
    "      is_training=False,\n",
    "      drop_remainder=False)\n",
    "\n",
    "  print(eval_filename)\n",
    "\n",
    "  # If running eval on the TPU, you will need to specify the number of steps.\n",
    "  all_results = []\n",
    "\n",
    "  for result in estimator.predict(\n",
    "      predict_input_fn, yield_single_examples=True):\n",
    "    if len(all_results) % 1000 == 0:\n",
    "      print(\"Processing example: %d\" % (len(all_results)))\n",
    "\n",
    "    unique_id = int(result[\"unique_ids\"])\n",
    "    start_logits = [float(x) for x in result[\"start_logits\"].flat]\n",
    "    end_logits = [float(x) for x in result[\"end_logits\"].flat]\n",
    "    answer_type_logits = [float(x) for x in result[\"answer_type_logits\"].flat]\n",
    "\n",
    "    all_results.append(\n",
    "        tf2baseline.RawResult(\n",
    "            unique_id=unique_id,\n",
    "            start_logits=start_logits,\n",
    "            end_logits=end_logits,\n",
    "            answer_type_logits=answer_type_logits))\n",
    "\n",
    "  print (\"Going to candidates file\")\n",
    "\n",
    "  candidates_dict = tf2baseline.read_candidates(FLAGS.predict_file)\n",
    "\n",
    "  print (\"setting up eval features\")\n",
    "\n",
    "  eval_features = [\n",
    "      tf.train.Example.FromString(r)\n",
    "      for r in tf.compat.v1.python_io.tf_record_iterator(eval_filename)\n",
    "  ]\n",
    "\n",
    "  print (\"compute_pred_dict\")\n",
    "\n",
    "  nq_pred_dict = tf2baseline.compute_pred_dict(candidates_dict, eval_features,\n",
    "                                   [r._asdict() for r in all_results])\n",
    "  predictions_json = {\"predictions\": list(nq_pred_dict.values())}\n",
    "\n",
    "  print (\"writing json\")\n",
    "\n",
    "  with tf.io.gfile.GFile(FLAGS.output_prediction_file, \"w\") as f:\n",
    "    json.dump(predictions_json, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, we turn `predictions.json` into a `submission.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_answers_df = pd.read_json(\"/kaggle/working/predictions.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bert model produces a `confidence` score, which the Kaggle metric does not use. You, however, can use that score to determine which answers get submitted. See the limits commented out in `create_short_answer` and `create_long_answer` below for an example.\n",
    "\n",
    "Values for `confidence` will range between `1.0` and `2.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_short_answer(entry):\n",
    "    # if entry[\"short_answers_score\"] < 1.5:\n",
    "    #     return \"\"\n",
    "    \n",
    "    answer = []    \n",
    "    for short_answer in entry[\"short_answers\"]:\n",
    "        if short_answer[\"start_token\"] > -1:\n",
    "            answer.append(str(short_answer[\"start_token\"]) + \":\" + str(short_answer[\"end_token\"]))\n",
    "    if entry[\"yes_no_answer\"] != \"NONE\":\n",
    "        answer.append(entry[\"yes_no_answer\"])\n",
    "    return \" \".join(answer)\n",
    "\n",
    "def create_long_answer(entry):\n",
    "   # if entry[\"long_answer_score\"] < 1.5:\n",
    "   # return \"\"\n",
    "\n",
    "    answer = []\n",
    "    if entry[\"long_answer\"][\"start_token\"] > -1:\n",
    "        answer.append(str(entry[\"long_answer\"][\"start_token\"]) + \":\" + str(entry[\"long_answer\"][\"end_token\"]))\n",
    "    return \" \".join(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_answers_df[\"long_answer_score\"] = test_answers_df[\"predictions\"].apply(lambda q: q[\"long_answer_score\"])\n",
    "test_answers_df[\"short_answer_score\"] = test_answers_df[\"predictions\"].apply(lambda q: q[\"short_answers_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    346.000000\n",
       "mean       1.165542\n",
       "std        0.177821\n",
       "min        0.000000\n",
       "25%        1.108867\n",
       "50%        1.177747\n",
       "75%        1.250758\n",
       "max        1.460662\n",
       "Name: long_answer_score, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_answers_df[\"long_answer_score\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of what each sample's answers look like in `prediction.json`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'example_id': '-1220107454853145579',\n",
       " 'long_answer': {'start_token': 18,\n",
       "  'end_token': 136,\n",
       "  'start_byte': -1,\n",
       "  'end_byte': -1},\n",
       " 'long_answer_score': 1.12259604036808,\n",
       " 'short_answers': [{'start_token': 70,\n",
       "   'end_token': 85,\n",
       "   'start_byte': -1,\n",
       "   'end_byte': -1}],\n",
       " 'short_answers_score': 1.12259604036808,\n",
       " 'yes_no_answer': 'NONE'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_answers_df.predictions.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We re-format the JSON answers to match the requirements for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_answers_df[\"long_answer\"] = test_answers_df[\"predictions\"].apply(create_long_answer)\n",
    "test_answers_df[\"short_answer\"] = test_answers_df[\"predictions\"].apply(create_short_answer)\n",
    "test_answers_df[\"example_id\"] = test_answers_df[\"predictions\"].apply(lambda q: str(q[\"example_id\"]))\n",
    "\n",
    "long_answers = dict(zip(test_answers_df[\"example_id\"], test_answers_df[\"long_answer\"]))\n",
    "short_answers = dict(zip(test_answers_df[\"example_id\"], test_answers_df[\"short_answer\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we add them to our sample submission. Recall that each sample has both a `_long` and `_short` entry in the sample submission, one for each type of answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"/kaggle/input/tensorflow2-question-answering/sample_submission.csv\")\n",
    "\n",
    "long_prediction_strings = sample_submission[sample_submission[\"example_id\"].str.contains(\"_long\")].apply(lambda q: long_answers[q[\"example_id\"].replace(\"_long\", \"\")], axis=1)\n",
    "short_prediction_strings = sample_submission[sample_submission[\"example_id\"].str.contains(\"_short\")].apply(lambda q: short_answers[q[\"example_id\"].replace(\"_short\", \"\")], axis=1)\n",
    "\n",
    "sample_submission.loc[sample_submission[\"example_id\"].str.contains(\"_long\"), \"PredictionString\"] = long_prediction_strings\n",
    "sample_submission.loc[sample_submission[\"example_id\"].str.contains(\"_short\"), \"PredictionString\"] = short_prediction_strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we write out our submission!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
